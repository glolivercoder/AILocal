#!/usr/bin/env python3
"""
Teste Completo do Agente de Gest√£o de Fluxo de MCPs
Demonstra√ß√£o de viabilidade e funcionalidades
"""

import json
import time
from pathlib import Path
from mcp_flow_agent import MCPFlowAgent, TaskResult

def test_prompt_analysis():
    """Testa an√°lise de prompts"""
    print("üîç Testando An√°lise de Prompts")
    print("=" * 50)
    
    agent = MCPFlowAgent()
    
    test_cases = [
        {
            "prompt": "Crie um arquivo de texto com o conte√∫do 'Hello World'",
            "expected_mcps": ["filesystem"],
            "description": "Tarefa simples de arquivo"
        },
        {
            "prompt": "Navegue para google.com e tire um screenshot da p√°gina",
            "expected_mcps": ["browser-tools"],
            "description": "Tarefa de navega√ß√£o web"
        },
        {
            "prompt": "Fa√ßa um commit no reposit√≥rio Git com a mensagem 'Update'",
            "expected_mcps": ["github"],
            "description": "Tarefa de controle de vers√£o"
        },
        {
            "prompt": "Execute uma query SQL no banco de dados PostgreSQL",
            "expected_mcps": ["postgres"],
            "description": "Tarefa de banco de dados"
        },
        {
            "prompt": "Use um modelo local do Ollama para analisar este texto",
            "expected_mcps": ["ollama"],
            "description": "Tarefa de IA local"
        },
        {
            "prompt": "Busque informa√ß√µes sobre Python no Google",
            "expected_mcps": ["google-maps"],
            "description": "Tarefa de busca"
        },
        {
            "prompt": "Crie um arquivo, navegue para um site e fa√ßa um commit",
            "expected_mcps": ["filesystem", "browser-tools", "github"],
            "description": "Tarefa complexa multi-MCP"
        }
    ]
    
    results = []
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nüìù Teste {i}: {test_case['description']}")
        print(f"Prompt: {test_case['prompt']}")
        
        analysis = agent.prompt_analyzer.analyze_prompt_intelligently(test_case['prompt'])
        
        print(f"‚úÖ MCPs detectados: {analysis.mcps_required}")
        print(f"üìä Prioridade: {analysis.priority}")
        print(f"‚è±Ô∏è  Dura√ß√£o estimada: {analysis.estimated_duration}s")
        print(f"üíæ Intensidade de recursos: {analysis.resource_intensity}")
        print(f"üéØ Confian√ßa: {analysis.confidence:.2f}")
        
        # Verificar acur√°cia
        accuracy = len(set(analysis.mcps_required) & set(test_case['expected_mcps'])) / len(test_case['expected_mcps'])
        results.append({
            "test": i,
            "description": test_case['description'],
            "accuracy": accuracy,
            "detected": analysis.mcps_required,
            "expected": test_case['expected_mcps']
        })
        
        print(f"üéØ Acur√°cia: {accuracy:.2f}")
    
    # Resumo dos resultados
    print("\nüìä Resumo da An√°lise de Prompts")
    print("=" * 50)
    avg_accuracy = sum(r['accuracy'] for r in results) / len(results)
    print(f"Acur√°cia m√©dia: {avg_accuracy:.2f}")
    
    for result in results:
        status = "‚úÖ" if result['accuracy'] >= 0.8 else "‚ö†Ô∏è" if result['accuracy'] >= 0.5 else "‚ùå"
        print(f"{status} Teste {result['test']}: {result['description']} - {result['accuracy']:.2f}")
    
    agent.shutdown()
    return avg_accuracy

def test_resource_monitoring():
    """Testa monitoramento de recursos"""
    print("\nüíª Testando Monitoramento de Recursos")
    print("=" * 50)
    
    agent = MCPFlowAgent()
    
    # Aguardar inicializa√ß√£o do monitoramento
    time.sleep(3)
    
    print("üìä M√©tricas do Sistema:")
    resources = agent.resource_monitor.get_available_resources()
    for key, value in resources.items():
        print(f"  {key}: {value:.1f}%")
    
    print("\nüîç Testando Crit√©rios de Recursos:")
    test_mcps = [
        ("filesystem", "baixa"),
        ("browser-tools", "m√©dia"),
        ("ollama", "alta")
    ]
    
    for mcp_name, intensity in test_mcps:
        can_start = agent.resource_monitor.can_start_mcp(mcp_name, intensity)
        status = "‚úÖ" if can_start else "‚ùå"
        print(f"{status} {mcp_name} ({intensity}): {'Pode iniciar' if can_start else 'Recursos insuficientes'}")
    
    agent.shutdown()

def test_learning_system():
    """Testa sistema de aprendizado"""
    print("\nüß† Testando Sistema de Aprendizado")
    print("=" * 50)
    
    agent = MCPFlowAgent()
    
    # Simular algumas tarefas para aprendizado
    test_tasks = [
        TaskResult(
            task_id="test_001",
            prompt="Criar arquivo simples",
            mcps_used=["filesystem"],
            success=True,
            duration=15.0,
            resource_usage={"cpu": 5.0, "memory": 10.0},
            performance_score=0.9
        ),
        TaskResult(
            task_id="test_002",
            prompt="Navega√ß√£o web",
            mcps_used=["browser-tools"],
            success=True,
            duration=45.0,
            resource_usage={"cpu": 15.0, "memory": 25.0},
            performance_score=0.8
        ),
        TaskResult(
            task_id="test_003",
            prompt="Tarefa complexa",
            mcps_used=["filesystem", "browser-tools", "github"],
            success=True,
            duration=120.0,
            resource_usage={"cpu": 30.0, "memory": 50.0},
            performance_score=0.7
        )
    ]
    
    print("üìö Registrando tarefas para aprendizado:")
    for task in test_tasks:
        agent.learning_system.learn_from_task(task)
        print(f"  ‚úÖ {task.task_id}: {len(task.mcps_used)} MCPs, score {task.performance_score:.2f}")
    
    # Testar otimiza√ß√£o
    print("\nüîç Testando Otimiza√ß√£o:")
    from mcp_flow_agent import MCPAnalysis
    
    analysis = MCPAnalysis(
        mcps_required=["filesystem", "browser-tools"],
        priority="m√©dia",
        estimated_duration=60.0,
        resource_intensity="m√©dia",
        confidence=0.8
    )
    
    available_resources = {"cpu_available": 80.0, "memory_available": 70.0}
    optimized = agent.learning_system.optimize_mcp_selection(analysis, available_resources)
    
    print(f"An√°lise original: {analysis.mcps_required}")
    print(f"Otimiza√ß√£o: {optimized}")
    
    agent.shutdown()

def test_complete_workflow():
    """Testa fluxo completo do agente"""
    print("\nüîÑ Testando Fluxo Completo")
    print("=" * 50)
    
    agent = MCPFlowAgent()
    
    # Testar tarefas simples
    simple_tasks = [
        "Crie um arquivo de teste",
        "Analise este prompt para determinar MCPs necess√°rios",
        "Verifique os recursos dispon√≠veis no sistema"
    ]
    
    results = []
    for i, prompt in enumerate(simple_tasks, 1):
        print(f"\nüîÑ Executando tarefa {i}: {prompt}")
        
        start_time = time.time()
        result = agent.process_task(prompt)
        end_time = time.time()
        
        print(f"  ‚è±Ô∏è  Tempo total: {end_time - start_time:.2f}s")
        print(f"  üéØ Sucesso: {'‚úÖ' if result.success else '‚ùå'}")
        print(f"  üìä Score: {result.performance_score:.2f}")
        print(f"  üîß MCPs usados: {result.mcps_used}")
        
        results.append(result)
    
    # Estat√≠sticas
    print("\nüìà Estat√≠sticas do Fluxo:")
    successful_tasks = sum(1 for r in results if r.success)
    avg_score = sum(r.performance_score for r in results) / len(results)
    avg_duration = sum(r.duration for r in results) / len(results)
    
    print(f"  Tarefas bem-sucedidas: {successful_tasks}/{len(results)}")
    print(f"  Score m√©dio: {avg_score:.2f}")
    print(f"  Dura√ß√£o m√©dia: {avg_duration:.2f}s")
    
    agent.shutdown()
    return results

def test_integration_with_existing_system():
    """Testa integra√ß√£o com sistema existente"""
    print("\nüîó Testando Integra√ß√£o com Sistema Existente")
    print("=" * 50)
    
    agent = MCPFlowAgent()
    
    # Verificar integra√ß√£o com MCPManager
    print("üìã Status do MCPManager:")
    mcp_status = agent.mcp_manager.get_mcp_status()
    print(f"  Total de MCPs configurados: {len(mcp_status)}")
    
    # Verificar MCPs dispon√≠veis
    available_mcps = [name for name, info in mcp_status.items() if info['status'] == 'ready']
    print(f"  MCPs dispon√≠veis: {len(available_mcps)}")
    
    # Testar an√°lise autom√°tica
    print("\nüîç Testando An√°lise Autom√°tica:")
    test_prompt = "Crie um arquivo e navegue para um site"
    auto_result = agent.mcp_manager.auto_manage_mcps(test_prompt)
    
    print(f"Prompt: {test_prompt}")
    print(f"MCPs sugeridos: {auto_result['suggested_mcps']}")
    print(f"MCPs iniciados: {auto_result['started']}")
    print(f"MCPs parados: {auto_result['stopped']}")
    
    if auto_result['errors']:
        print(f"Erros: {auto_result['errors']}")
    
    agent.shutdown()

def generate_performance_report():
    """Gera relat√≥rio de performance"""
    print("\nüìä Relat√≥rio de Performance")
    print("=" * 50)
    
    # Executar todos os testes
    print("üöÄ Executando bateria de testes...")
    
    # Teste 1: An√°lise de prompts
    prompt_accuracy = test_prompt_analysis()
    
    # Teste 2: Monitoramento de recursos
    test_resource_monitoring()
    
    # Teste 3: Sistema de aprendizado
    test_learning_system()
    
    # Teste 4: Fluxo completo
    workflow_results = test_complete_workflow()
    
    # Teste 5: Integra√ß√£o
    test_integration_with_existing_system()
    
    # Gerar relat√≥rio final
    print("\nüéØ RELAT√ìRIO FINAL DE VIABILIDADE")
    print("=" * 60)
    
    # M√©tricas de sucesso
    successful_workflows = sum(1 for r in workflow_results if r.success)
    workflow_success_rate = successful_workflows / len(workflow_results) if workflow_results else 0
    
    print(f"‚úÖ An√°lise de Prompts: {prompt_accuracy:.2f} de acur√°cia")
    print(f"‚úÖ Monitoramento de Recursos: Funcionando")
    print(f"‚úÖ Sistema de Aprendizado: Funcionando")
    print(f"‚úÖ Fluxo Completo: {workflow_success_rate:.2f} de sucesso")
    print(f"‚úÖ Integra√ß√£o com Sistema: Funcionando")
    
    # Avalia√ß√£o geral
    overall_score = (prompt_accuracy + workflow_success_rate) / 2
    
    print(f"\nüéØ Score Geral: {overall_score:.2f}")
    
    if overall_score >= 0.8:
        print("üöÄ VIABILIDADE: ALTA - Sistema pronto para produ√ß√£o")
    elif overall_score >= 0.6:
        print("‚ö†Ô∏è  VIABILIDADE: M√âDIA - Necessita melhorias menores")
    else:
        print("‚ùå VIABILIDADE: BAIXA - Necessita desenvolvimento significativo")
    
    # Recomenda√ß√µes
    print(f"\nüí° RECOMENDA√á√ïES:")
    if prompt_accuracy < 0.8:
        print("  - Melhorar an√°lise de prompts com LLM")
    if workflow_success_rate < 0.8:
        print("  - Otimizar gerenciamento de MCPs")
    if overall_score >= 0.8:
        print("  - Implementar interface gr√°fica")
        print("  - Adicionar mais MCPs")
        print("  - Otimizar performance")
    
    return overall_score

if __name__ == "__main__":
    print("üß™ TESTE COMPLETO DO AGENTE DE FLUXO DE MCPS")
    print("=" * 60)
    
    try:
        score = generate_performance_report()
        print(f"\n‚úÖ Teste conclu√≠do com score: {score:.2f}")
    except Exception as e:
        print(f"\n‚ùå Erro durante o teste: {e}")
        import traceback
        traceback.print_exc() 